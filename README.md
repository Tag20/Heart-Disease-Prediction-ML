# Heart-Disease-Prediction-ML
Welcome to my comprehensive repository for heart disease prediction! Leveraging Python's data processing and machine learning libraries like Pandas, NumPy, and scikit-learn, this repository offers a thorough exploration of heart disease prediction using various classification algorithms. From preprocessing the dataset to evaluating model performance with metrics like accuracy, F1-score, and ROC curves, I've covered it all. You'll find implementations of logistic regression, K-nearest neighbors, decision trees, and random forest classifiers, along with techniques like outlier removal, feature scaling, and hyperparameter tuning. Feel free to explore the code, learn from the detailed comments, and utilize the predictive models for your own heart disease prediction tasks.
For Data Analysis we used data visualization tools like Box Plots which includes Inter Quartile Range(IQR) and dropped all outliers in our dataset. Post that we used One Hot Encoding technique to convert all categorical variables into a format that can help us provide greater accuracy to ML algorithms. We also normalized the data to ensure that the features are on the same scale to prevent the dominance on any single feature and also to ensure equal contribution of features in the distance based algorithm we were going to use.

Our first ML model is Logistics Regression which is a supervised learning algorithm that is used for binary classification tasks. Despite its name it is primarily used for classification and not regression. The central idea behind Logistic Regression is to model the probability that an observation belongs to a particular class. It uses a linear relationship between the input features and the log-odds of the outcome. The accuracy we had on our test data was 81.33%. For visualizing we protted the Logistic Regression line and the ROC Curve

The next ML model we use is KNN which is an abbrevation for K Nearest Neighbours. It is an versatile and straighforward surpervised ML algorithm that is used for both classification and regression tasks. It is based on some distance metric usually Euclidean distance or City Block Distance or even maybe Manhatten Distance. Here we had bad training scores for the model at 54.7% but good testing accuracy score at 83.33%
Moving on to sececting our K value using Elbow Method. Elbow method is a heuristic approach which is used to select the optimal number of clusters in the KNN.

The last ML model we used is Decision Tree Classification. They are hierarchical structures consisting of nodes that represent features, and edges that represent decisions or rules, and leaves that represent the outcome or class lable. The algorithm learns a tree-like structure by recursively partitioning the features into space or regions that minimize the impurity or maximize the information gain. We use entroy to calculate the information gain and split the tree accordinging to the maximum information gain.

<p align="center">
<img src="https://raw.githubusercontent.com/github/explore/80688e429a7d4ef2fca1e82350fe8e3517d3494d/topics/python/python.png" alt="Python" height="40" style="vertical-align:top; margin:4px">
</p>
